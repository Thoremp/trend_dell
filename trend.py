#coding=utf-8

import urllib
from lxml import etree
import htmllist
import trendUtil
import re
import time
import sys
import logging
from multiprocessing.dummy import Pool
reload(sys)
sys.setdefaultencoding('utf-8')
"""
    ä¿¡æ¯é¡ºåº:æ ‡é¢˜,å…¬å¸åå­—,å…¬å¸ç½‘å€,æœˆè–ª,å·¥ä½œåœ°ç‚¹,å‘å¸ƒæ—¥æœŸ,å·¥ä½œæ€§è´¨,å·¥ä½œç»éªŒ,æœ€ä½å­¦å†,æ‹›è˜äººæ•°,èŒä½ç±»åˆ«
    æœ€é‡è¦çš„æ˜¯äººæ•°å’ŒèŒä½ç±»åˆ«
"""

# å¾ªç¯æœç´¢æ‰€æœ‰ç±»åˆ« è¿”å›æ•°ç»„urls-102ä¸ªurl
def getAllUrls():

    # logging.info("å¼€å§‹!")
    # å£°æ˜ä¸€ä¸ªæ•°ç»„å­˜æ”¾æ‰€æœ‰çš„ url é›†åˆ
    urls = []

    # å˜æ¢ bjå±æ€§å’Œ sjå±æ€§ è·å¾—æ‰€æœ‰èŒä½åˆ†ç±»çš„ url
    # 1.ä»æ•°æ®åº“è·å¾—æ‰€æœ‰çš„ bjå’Œ sj(ç›¸å¯¹åº”çš„)
    result = trendUtil.getClassAndCode()
    # 2.å¾ªç¯èµ‹å€¼è·å¾— url
    for each in result:
        sj = each[0]
        bj = each[1]
        params = urllib.urlencode({'bj':bj,'sj':sj,'p':1,'isadv':0,'jl':'å…¨å›½'})
        url = 'http://sou.zhaopin.com/jobs/searchresult.ashx?%s' % params
        urls.append(url)
    return urls

# è·å–æ‰€æœ‰è¯¦ç»†ä¿¡æ¯çš„url è¾“å‡ºurlIns
def printUrl(url):

    print url + " - " + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))

    try:
        html = urllib.urlopen(url, data=None, timeout=3).read()
    except:
        print "è§£æurlå‡ºç°é”™è¯¯:" + url
    else:
        selector = etree.HTML(html)
        # è·å–å†…éƒ¨è¯¦ç»†ä¿¡æ¯çš„ç½‘å€
        secondurl = selector.xpath('//td[@class="zwmc"]/div/a/@href')

        for each in secondurl:
            if each == 'http://xiaoyuan.zhaopin.com/zhuanti/first.html':
                continue
            if each == 'http://e.zhaopin.com/products/1/detail.do':
                continue
            if each == 'http://xiaoyuan.zhaopin.com/first/':
                continue
            if each[0:27] == 'http://xiaoyuan.zhaopin.com':
                continue

            print each + " - " + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))
            #ä¿å­˜æ•°æ®åº“ä¹‹å‰æŸ¥ä¸€ä¸‹åœ¨trend_basisä¸­æ˜¯å¦å·²ç»å­˜åœ¨è¯¥url,å¦‚æœå­˜åœ¨å°±ä¸ä¿å­˜äº†
            count = trendUtil.judgeUrlBasis(each)
            if(count == 0):
                #ä¿å­˜åˆ°æ•°æ®åº“
                trendUtil.saveAllUrl(each)


    # è·å–ä¸‹ä¸€é¡µæŒ‰é’®çš„ç½‘å€(å¦‚æœæœ‰çš„è¯)
    try:
        content = selector.xpath('//li[@class="pagesDown-pos"]/a/@href')
    except:
        print "è§£æä¸‹ä¸€é¡µurlå‡ºç°é”™è¯¯:" + url
        # logging.info("è§£æä¸‹ä¸€é¡µurlå‡ºç°é”™è¯¯:" + url)
    else:
        if content:
            content = content[0]
            printUrl(content)
        else:
            print "end"

# å¤„ç†è¯¦ç»†ä¿¡æ¯é¡µ,å­˜å‚¨æ•°æ®åº“
def saveUrl(each):

    each = each[1]
    print "æ­£åœ¨ä¿å­˜:" + each + " - " + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))
    # logging.info("æ­£åœ¨ä¿å­˜:" + each + " - " + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())))

    # å­˜å‚¨æ•°æ®åº“ä¹‹å‰å…ˆæ£€æŸ¥æ•°æ®åº“æœ‰æ²¡æœ‰è¿™æ¡ä¿¡æ¯,å¦‚æœæœ‰åˆ™ä¸å­˜äº†
    result = trendUtil.isBasisByUrl(each)
    if result == 'have':
        print "æ•°æ®åº“å·²å­˜åœ¨æ­¤æ•°æ®:" + each
        # logging.info("æ•°æ®åº“å·²å­˜åœ¨æ­¤æ•°æ®:" + each)
    else:

        try:
            item = innerHtml(each)
        except:
            print "è§£æurlå¤±è´¥:" + each
            # logging.info("è§£æurlå¤±è´¥:" + each)
        else:

            try:
                trendUtil.saveTrendBasis(item)  # å­˜å‚¨æ•°æ®åº“
            except:
                print "ä¿å­˜å¤±è´¥:" + each
                # logging.info("ä¿å­˜å¤±è´¥:" + each)
            else:
                print "ä¿å­˜æˆåŠŸ:" + each
                # logging.info("ä¿å­˜æˆåŠŸ:" + each)

    # åœ¨ trend_url ä¸­å°†æ­¤ url çš„ status ç½®ä¸º 1
    trendUtil.setStatusByUrl(each)

# è·å–è¯¦ç»†ä¿¡æ¯(ä»å†…éƒ¨è¯¦ç»†ä¿¡æ¯ç½‘é¡µ) è¿”å›æ•°ç»„item
def innerHtml(url):
    "éœ€è¦è¾“å…¥è¿›å…¥è¯¦ç»†é¡µé¢çš„ç½‘å€,å¯¹å†…éƒ¨é¡µé¢è¯»å–è¯¦ç»†ä¿¡æ¯"
    item = []
    item.append(url)
    # htmlSecond = htmllist.html1 # æµ‹è¯•ç”¨
    htmlSecond = urllib.urlopen(url).read() #çœŸå®ä»£ç 

    # æ ‡é¢˜
    selectorSecond = etree.HTML(htmlSecond)
    position = selectorSecond.xpath('//h1/text()')
    if position:
        position = position[0]
        position = position.replace('ğŸ”´', '')
    item.append(position)

    # å…¬å¸åç§° å’Œ å…¬å¸ä¸»é¡µurl
    company = selectorSecond.xpath('//h2/a/text()')
    companyUrl = selectorSecond.xpath('//h2/a/@href')
    if company:
        company = company[0]
    if companyUrl:
        companyUrl = companyUrl[0]
    item.append(company)
    item.append(companyUrl)

    selectorThird = selectorSecond.xpath('//div[@class="terminalpage clearfix"]/div[@class="terminalpage-left"]/ul/li')
    time.sleep(0.5)
    # å·¥èµ„
    name = selectorThird[0].xpath('span/text()')
    value = selectorThird[0].xpath('strong/text()')
    if name:
        name = name[0]
    if value:
        value = value[0]
    item.append(value)

    # å·¥ä½œåœ°å€
    name = selectorThird[1].xpath('span/text()')[0]
    value = selectorThird[1].xpath('strong/a/text()')[0]
    item.append(value)

    # å‘å¸ƒæ—¥æœŸ
    name = selectorThird[2].xpath('span/text()')[0]
    value = selectorThird[2].xpath('strong/span/text()')[0]
    # value = re.findall('<strong><span id="span4freshdate">(.*?)</span></strong>',htmlSecond,re.S)
    item.append(value)

    # å·¥ä½œæ€§è´¨
    name = selectorThird[3].xpath('span/text()')[0]
    value = selectorThird[3].xpath('strong/text()')[0]
    item.append(value)

    #å·¥ä½œç»éªŒ
    name = selectorThird[4].xpath('span/text()')[0]
    value = selectorThird[4].xpath('strong/text()')[0]
    item.append(value)

    # æœ€ä½å­¦å†
    name = selectorThird[5].xpath('span/text()')[0]
    value = selectorThird[5].xpath('strong/text()')[0]
    item.append(value)

    # æ‹›è˜äººæ•° 30äºº
    name = selectorThird[6].xpath('span/text()')[0]
    value = selectorThird[6].xpath('strong/text()')[0]
    value1 = re.findall('(\d+)',value,re.S)
    item.append(value1[0])

    # èŒä½ç±»åˆ«
    name = selectorThird[7].xpath('span/text()')[0]
    value = selectorThird[7].xpath('strong/a/text()')[0]
    item.append(value)

    # å…¬å¸ä¸€äº›ä¿¡æ¯
    selectorThird = selectorSecond.xpath('//div[@class="company-box"]/ul/li')
    # å…¬å¸è§„æ¨¡ï¼š100-499äººå…¬å¸
    name = selectorThird[0].xpath('span/text()')[0]
    value = selectorThird[0].xpath('strong/text()')[0]
    item.append(value)

    # æ€§è´¨ï¼šè‚¡ä»½åˆ¶ä¼ä¸šå…¬å¸
    name = selectorThird[1].xpath('span/text()')[0]
    value = selectorThird[1].xpath('strong/text()')[0]
    item.append(value)

    # è¡Œä¸šï¼šè®¡ç®—æœºè½¯ä»¶
    name = selectorThird[2].xpath('span/text()')[0]
    value = selectorThird[2].xpath('strong/a/text()')[0]
    item.append(value)

    # å…¬å¸åœ°å€ï¼š åŒ—äº¬å¸‚æµ·æ·€åŒºä¸°æƒ ä¸­è·¯7å·æ–°ææ–™åˆ›ä¸šå¤§å¦
    name = selectorThird[3].xpath('span/text()')[0]
    name = name.replace(':',"")[0:4]
    if name == 'å…¬å¸åœ°å€':
        value = selectorThird[3].xpath('strong/text()')[0]
        value = value.strip()
        item.append(value)
    else:
        value = selectorThird[4].xpath('strong/text()')[0]
        value = value.strip()
        item.append(value)
    return item

# ä¿å­˜æ‰€æœ‰è¯¦ç»†ä¿¡æ¯url
def saveInurl(each):
    urls= []
    print "æ­£åœ¨ä¿å­˜:" + each
    html = urllib.urlopen(each).read()
    selector = etree.HTML(html)
    secondurl = selector.xpath('//td[@class="zwmc"]/div/a/@href')
    for e in secondurl:
        urls.append(e)
    return urls

if __name__ == '__main__':

    # while 1:
    #     try:
    #         # 2.è§£æurl(ä½¿ç”¨å¤šçº¿ç¨‹)
    #         trendUtil.deleteTrendUrl() # åˆ é™¤ä¸åˆæ ¼çš„url
    #         result = trendUtil.getUrlBystatus()
    #         pool = Pool()  # åˆå§‹åŒ–ä¸€ä¸ªå®ä¾‹,4ä»£è¡¨å››çº¿ç¨‹,ä¸å†™å‚æ•°å°†ä½¿ç”¨æœºå™¨æ ¸å¿ƒæ•°ä½œä¸ºå‚æ•°
    #         pool.map(saveUrl, result)  # å®ç°çˆ¬å–
    #         pool.close()
    #         pool.join()  # join()ä½œç”¨:ç­‰å¾…çˆ¬å–å®Œæˆä»¥åå†å›åˆ°ä¸‹é¢ä¸€è¡Œä»£ç çš„æ‰§è¡Œ
    #     except:
    #         print u'å¤šçº¿ç¨‹å‡ºç°é”™è¯¯ - ' + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))
    #         continue
    #
    #     try:
    #         # 1.å–å‡ºurl å¹¶ä¿å­˜åˆ°æ•°æ®åº“ æ¯å¤©ä¸€æ¬¡
    #         urls = getAllUrls()
    #         pool = Pool()  # åˆå§‹åŒ–ä¸€ä¸ªå®ä¾‹,4ä»£è¡¨å››çº¿ç¨‹,ä¸å†™å‚æ•°å°†ä½¿ç”¨æœºå™¨æ ¸å¿ƒæ•°ä½œä¸ºå‚æ•°
    #         pool.map(printUrl, urls)
    #         pool.close()
    #         pool.join()  # join()ä½œç”¨:ç­‰å¾…çˆ¬å–å®Œæˆä»¥åå†å›åˆ°ä¸‹é¢ä¸€è¡Œä»£ç çš„æ‰§è¡Œ
    #     except:
    #         print u'å¤šçº¿ç¨‹å‡ºç°é”™è¯¯ - ' + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))
    #         continue

    trendUtil.deleteTrendUrl()  # åˆ é™¤ä¸åˆæ ¼çš„url
    result = trendUtil.getUrlBystatus_dell()
    pool = Pool()
    pool.map(saveUrl, result)
    pool.close()
    pool.join()